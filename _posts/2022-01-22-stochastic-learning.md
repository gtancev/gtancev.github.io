---
title: 'Stochastic Learning'
date: 2022-01-22 00:00:00
description: Stochastic Learning
featured_image: '/images/posts/machine-learning/google-deepmind.jpg'
---

![](/images/posts/machine-learning/google-deepmind.jpg)

## Supervised Learning

Calibration of sensor systems & networks is essentially supervised learning. The following post looks at the theoretical aspects and motivates [stochastic online calibration](https://ieeexplore.ieee.org/abstract/document/9690889).

### Linear Models, Neural Networks, and Gradient-Based Optimization

Once a data set $\mathbf{X} = [\mathbf{x}_1, ..., \mathbf{x}_p] = [x_1, ..., x_n]^T \in \mathbb{R}^{n\times p}$ in $n$ samples and $p$ independent variables (or features) $X_i \in \mathcal{X}$, $i\in\{1, ..., p\}$ is collected (with $n > p$), the influence of the independent variables on the dependent variables $Y_k \in \mathcal{Y}$, $k\in\{1, ..., q\}$ can be evaluated. In this notation, $\mathbf{X}$ contains a column with 1's for the intercept. The aim is to find a function $f$ that maps from $\mathcal{X}$ to $\mathcal{Y}$, i.e., $f: \mathcal{X}\to\mathcal{Y}$. This is a supervised learning problem because the output is known. If the output is also continuous ($Y \in \mathbb{R}$), this particular type of task is called regression. For instance, calibrating a low-cost sensor system is a typical regression problem.

Let the vector $\mathbf{y} = [y_1, ..., y_n]^T \in \mathbb{R}^{n}$ be the noisy realization of $Y$ generated by the model $\mathbf{y} = \phi(\mathbf{X}w) + \boldsymbol\epsilon$ with $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ for $i \in \{1, ..., n\}$, $w \in\mathbb{R}^{p}$ the model parameters, and $\phi(\cdot)$ an activation function. For the identity activation function $\phi(z) = z$, the model becomes $\mathbf{y} = \mathbf{X}w+\boldsymbol\epsilon$. As there are some degrees of freedom, the solution to this system of equations, $\hat{w}$, is obtained by minimizing the sum of squared differences between the the predicted (i.e., model output) and the observed values in Eq. \ref{mse}.

\begin{equation}
\label{mse}
\hat{w} = \arg \min_{w} \underbrace{(\mathbf{y}-\mathbf{X}w)^T(\mathbf{y}-\mathbf{X}w)}_{\mathcal{L}(w)}
\end{equation}

The loss function $\mathcal{L}$ can be motivated from an algebraic perspective by seeking a suitable basis in $\mathcal{X}$ for the expression of vector $\mathbf{y}$ (projection onto $\mathcal{X}$), but it also arises when minimizing the negative logarithmic likelihood of the observed data. The framework is known as linear regression because it is linear with respect to the model parameters (but not the linear activation function or variables). The closed-form solution is obtained by differentiating with respect to $w$ and setting the derivative to zero; it is given by Eq. \ref{lin_reg_sol}.

\begin{equation}
\label{lin_reg_sol}
\hat{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}

Solving the system in this manner is not always possible because an analytical solution does not exist in every case. Fortunately, the problem is also solvable in an iterative manner by means of gradient descent (see figures below). The main idea is to move along the loss landscape towards the steepest direction (i.e., the gradient) in small steps $t$ until reaching an optimum. To do so, the objective function must be differentiable; also, a starting point $w_{(0)}$ and a learning rate $\gamma$ are required. The update rule in Eq. \ref{sgd} is then applied until convergence.

\begin{equation}
\label{sgd}
w_{(t+1)} \gets w_{(t)} - \gamma\left.\frac{\partial \mathcal{L}}{\partial w}\right\vert_{w=w_{(t)}} = w_{(t)} - \gamma\mathbf{X}^T(\mathbf{y}-\mathbf{X}w_{(t)})
\end{equation}

<div class="gallery" data-columns="2">
	<img src="/images/theory/gradient.png">
	<img src="/images/theory/loss_NN.png">
</div>

Instead of using the complete data set of $n$ samples, a mini-batch or only the $k$-th random sample $x_{k}^T$ can be picked and the update is performed only with instance. This is known as stochastic gradient descent. Updating parameters with single observations is reasonable in situations in which new data are becoming available, i.e., in [online learning](https://en.wikipedia.org/wiki/Online_machine_learning), and when models need to be updated due to [changes in the underlying probability distributions](https://en.wikipedia.org/wiki/Concept_drift). If the underlying relationships are non-linear, it is beneficial to add additional variables to $\mathbf{X}$ via basis expansion ($X_i^2$, $X_iX_j$, etc). If interpretability is not crucial, one can apply a cascade of $L-1$ non-linear transformations instead, e.g., the sigmoid $\phi(z) = \frac{1}{1+e^{-z}} = \sigma(z)$, and a last affine ("linear") transformation, each parameterized by a set of parameters $\mathbf{W}^{[l]} \in \mathbb{R}^{n_{l}\times n_{l-1}}$ for transformation $l$ (Eq. \ref{nn}). The approach is supported by an [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) and the resulting model is known as multilayer perceptron or feedforward artificial neural network.

\begin{equation}
\label{nn}
\mathbf{y}^T = \phi_{\mathbf{W}^{[L]}} \circ \phi_{\mathbf{W}^{[L-1]}} \circ ... \circ \phi_{\mathbf{W}^{[1]}}(\mathbf{X}^T) +\boldsymbol\epsilon^T = \mathbf{W}^{[L]}\sigma(\mathbf{W}^{[L-1]}\sigma(...\sigma(\mathbf{W}^{[1]}\mathbf{X}^T)))+\boldsymbol\epsilon^T
\end{equation}

The variables $L$ and $n_l$ for $l\in\{1, ... , L-1\}$ are design parameters. Decomposing the expression from above, with $\mathbf{Z}^{[0]} = \mathbf{X}^T$ and $\mathbf{Z}^{[L]} = \mathbf{y}^T$, results in the intermediate values $\mathbf{Z}^{[l]}$ of the hidden nodes in layers $l\in \{1, ..., L\}$. Eqs. \ref{nn1} & \ref{nn2} are the so-called forward pass.

\begin{align}
\label{nn1}
\mathbf{A}^{[l]} = \mathbf{W}^{[l]}\mathbf{Z}^{[l-1]}
\end{align}

\begin{align}
\label{nn2}
\mathbf{Z}^{[l]} = \sigma(\mathbf{A}^{[l]})
\end{align}

The loss (Eq. \ref{mse}) remains but an analytical solution does not exist. The algorithm to optimize the model parameters is a variant of gradient descent (with an excessive use of the chain rule) known as [backpropagation](https://en.wikipedia.org/wiki/Backpropagation). Eqs. \ref{backprop1} & \ref{backprop2} describe the backward pass.

\begin{align}
\label{backprop1}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}} \frac{\partial \mathbf{Z}^{[L]}}{\partial \mathbf{A}^{[L]}} \frac{\partial \mathbf{A}^{[L]}}{\partial \mathbf{Z}^{[L-1]}}...\frac{\partial \mathbf{Z}^{[l+1]}}{\partial \mathbf{A}^{[l]}}  \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{W}^{[l]}}
\end{align}

\begin{align}
\label{backprop2}
\mathbf{W}^{[l]} \gets \mathbf{W}^{[l]} - \gamma \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}}
\end{align}

![Multilayer perceptron.](/images/posts/machine-learning/mlp.png)
