---
title: 'Model-Free Reinforcement Learning for Chemical Process Development'
subtitle: 'Moving towards universal chemical process operators.'
date: 2019-09-01 00:00:00
description: Model-Free Reinforcement Learning for Chemical Process Development
featured_image: '/images/projects/rl/1.jpg'
---

![](/images/projects/rl/1.jpg)

## Introduction

I developed the idea for this (personal) project after my continuing education in computer science with a focus in artificial intelligence. I asked myself how I could incorporate what I had learned into my domain. **Process development**, **design**, **optimization**, and **control** are, for instance, some of the main duties within chemical and process engineering. In concrete terms, the scope is finding an optimal recipe or suitable configuration of equipment or process parameters (via laboratory experiments) so that certain objectives (e.g., yield or throughput) are maximized while potential constraints (e.g., input concentrations, flow rates, reactor volumes, or boiling points of solvents) are respected. By **automating** these tasks, e.g., through laboratory robots, a great deal of manual labor could be saved.

The recent progress in reinforcement learning (RL) made it clear that agents can [master complex tasks](https://www.nature.com/articles/nature14236) and [play a variety of games](https://www.nature.com/articles/s41586-020-03051-4), or even discover more efficient (mathematical) procedures, e.g., for [matrix operations](https://www.nature.com/articles/s41586-022-05172-4) or [sorting](https://www.nature.com/articles/d41586-023-01883-4). With the availability of kinetic parameters, either from experiments or numerical simulations, agents may find optimal configurations and synthesis recipes. In contrast to convex optimization, however, the algorithm/model can be directly used for process control. Such experiments can take place either on the computer or directly in the laboratory, depending on the sample efficiency of the method, i.e., the number of interactions required for convergence. In the long term, this would (partly) automate process development.

[The foundations of RL, the benefits and limitations of such methods applied to chemical process control, as well as open problems have been recently reviewed](https://www.sciencedirect.com/science/article/abs/pii/S0098135419300754?via%3Dihub=). Meanwhile, several interesting applications in process engineering have been found. [One of these studies attempted to optimize a batch bio-process with a recurrent neural network using REINFORCE](https://towardsdatascience.com/model-free-reinforcement-learning-for-chemical-process-development-67318da35861). [Another study made use of Q-learning for process optimization while also trying to enforce constraints](https://www.sciencedirect.com/science/article/abs/pii/S0098135421002404?via%3Dihub=). [Others tried to schedule chemical production by means of advantage actor-critic](https://www.sciencedirect.com/science/article/pii/S0098135420301599?via%3Dihub=). The scope of the following article is to illustrate this on the synthesis of paracetamol using [proximal policy optimization (PPO)](https://arxiv.org/abs/1707.06347).

## Problem Definition

We have a computer program, a so-called agent, here we call it an universal chemical process operator. This operator finds itself in an environment in which it can perform chemical operations, i.e., actions. Such actions include dosing component A, increasing/decreasing input/output flow, increasing/decreasing temperature, and so on. As the agent perform actions in certain states such as concentrations of certain components, it transitions into new states.

For example, paracetamol (PC) is synthesized from p-aminophenol (AP) and acetic anhydride (AA), shown in Fig. 1a. Under known [kinetics](https://jurnal.ugm.ac.id/jrekpros/article/view/64551), this process can be modeled and represents the environment, e.g., in a continuous stirred-tank reactor (CSTR) as shown in Fig. 1b.

<center>
<figure>
<div class="gallery" data-columns="2">
	<img src="/images/projects/rl/reaction.png">
    <img src="/images/projects/rl/cstr.png">
</div>
<figcaption><b>Fig. 1: (a)</b> Reaction of interest. <b>(b)</b> Schema of a continuous stirred-tank reactor, with input/output flow, input concentrations, reactor concentrations, and volume.</figcaption>
</figure>
</center>

In each state, the agent picks a set of actions and the system of differential equations (representing the environment) is numerically integrated for a fixed time interval, resulting in a new state. The agent then receives a reward proportional to the amount of paracetamol synthesized. In other words, the agent plays a game and has to produce as much paracetamol as possible in a given period of time (here roughly 2,000 transitions or 170 hours). In the following study, the reactor is essentially a [laboratory-scale CSTR](https://pubs.rsc.org/en/content/articlelanding/2023/re/d2re00476c) with a maximum volume of 0.45 L. The maximum possible flow rates are 2.5 mL/min, and the maximum possible input concentrations are 3.3 mol/L. If the volume is exceeded, the agent receives a penalty proportional to the excess volume; if it exceeds a critical threshold, the episode ends immediately. The temperature is kept constant at 105 Â°C.

## Methods

In (deep) RL, the scope is to learn a policy (i.e., a mapping from states to actions) that maximizes the cumulative (discounted) sum of rewards. In each state, an action is performed, which then leads to a specific reward. PPO is a heuristic variant of [trust region policy optimization (TRPO)](http://proceedings.mlr.press/v37/schulman15.html). Both are so-called [actor-critic methods](https://proceedings.neurips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html) that make use of [function approximation](https://papers.nips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html). An actor-critic approach consists of two components; the actor is essentially a mapping from states to actions, parametrized by a (deep) neural network. It takes as input a state (i.e., an observation) and provides the best action to be played that state. In the following, the action distribution is represented by a [continuous Bernoulli distribution](https://papers.nips.cc/paper_files/paper/2019/hash/f82798ec8909d23e55679ee26bb26437-Abstract.html). This distribution provides values in the range of [0, 1], which then need to be multiplied by the maximum possible setting of each action. Such a stochastic policy allows a trade-off between [exploration and exploitation](https://arxiv.org/abs/2305.08624).

This choice of the "best" action is guided by the critic, which estimates the value (or the [advantage](https://arxiv.org/abs/1506.02438)) of each action in each state by means of another neural network, and it updates those estimates over the course of the training. Actions that lead to high rewards are encouraged, with the opposite being true for action leading to low rewards or even penalties. Suitable architectures and hyperparameters (e.g., for [optimization](https://ui.adsabs.harvard.edu/abs/2018arXiv181002525H/abstract)) are taken from the [literature](https://ojs.aaai.org/index.php/AAAI/article/view/11694).

## Results and Discussion

Fig. 2 illustrates the average reward as a function of the number of epochs for different hyperparameters, the base case being $\gamma$=0.995, $\lambda$=0.97, and $\epsilon$=0.20. The average reward in this plot is directly related to the manufactured amount of paracetamol.

<center>
<figure>
<img src="/images/projects/rl/avg_reward.png" width="600">
<figcaption><b>Fig. 2: </b> Learning curve for different hyperparameter combinations.</figcaption>
</figure>
</center>

The optimal policy, i.e., the one after convergence, is shown in Fig. 3.

<center>
<figure>
<img src="/images/projects/rl/epoch_1800.png" width="1800">
<figcaption><b>Fig. 3: </b> States and actions as a function of time (after 2000 epochs). Plot shows concentrations, volume, inlet concentrations, and flows as a function of operation time.</figcaption>
</figure>
</center>

It can be seen how the agent fills up the CSTR by keeping the ouput flow ($Q_2$) lower than the input flow ($Q_1$). As for the input concentrations, those are kept at the maximum from the beginning. In other words, the agent has learned how best to synthesize PA from interaction with the environment. This policy was elaborated on the computer because model-free RL schemes are sample inefficient. With more efficient (off-policy) schemes from model-based RL, chemical robots could develop syntheses independently in the laboratory in the future. However, this also requires [safe exploration](https://dl.acm.org/doi/10.5555/3454287.3454547).

## Conclusion and Outlook

This brief study indicates that RL certainly has potential for chemical process development and deserves further exploration. The potential is great, because laborious laboratory work could thus be handled by robots. However, many questions remain unanswered here. Model-free RL is sample-inefficient and model-based and/or off-policy methods (e.g., [soft actor-critic](https://arxiv.org/abs/1801.01290)) would be preferred. Furthermore, PPO is frequently reported to suffer from [instabilities](https://arxiv.org/abs/2009.10897). Finally, there is hardly any reaction taking place at low temperatures and thus no feedback in the form of rewards (i.e., sparse rewards), so suitable solutions must be found for this as well.